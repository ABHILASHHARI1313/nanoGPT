{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L,dim_v,dim_k = 4,8,8 # L = 4 Since we are using only 4 words for illustration. dim_v and dim_k are set to 8(Just for illustration).count\n",
    "\n",
    "#Setting q,v,k\n",
    "q = np.random.randn(L,dim_k)\n",
    "k = np.random.randn(L,dim_k)\n",
    "v = np.random.randn(L,dim_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here each row of the vectors(Query,Key,Value) corresponds to each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Query Vector q =========================\n",
      "[[ 0.27710041  0.03188016  1.44596882 -1.34205436  0.14321269  0.27645839\n",
      "   0.2120455  -0.03424386]\n",
      " [-1.68562268  1.35760768  0.30797969 -2.00162779 -0.88145719 -1.16567825\n",
      "  -0.42736467 -0.86933783]\n",
      " [ 0.97788076 -0.08125846 -1.12267725  1.27057746 -0.03749489  0.37929365\n",
      "  -1.45820779  0.2642624 ]\n",
      " [ 0.74134556 -1.02667713 -0.48092183  1.50016204 -0.18407928  0.49035576\n",
      "   0.43729467 -0.60433436]]\n",
      "===================== Key Vector k ===========================\n",
      "[[-1.09750997  0.60600674 -1.34702431 -0.00707761 -0.32464676  0.87536513\n",
      "   0.93560748  0.20497881]\n",
      " [-1.20536592  0.7792677   0.29531355 -0.80187422 -1.30155823 -0.62707862\n",
      "   0.63349846 -0.30552488]\n",
      " [-0.61109375 -1.48863168  1.2227034   0.50665623 -2.35235382 -0.86774457\n",
      "   0.35195043 -0.33173589]\n",
      " [ 0.30374713 -1.57928372 -0.63721208  1.94223431  0.32633268 -1.55898264\n",
      "  -1.68295535  0.11461672]]\n",
      "===================== Value Vector v =========================\n",
      "[[-0.4981263  -0.11508445  0.8109274   0.10882082 -0.52270394  1.20622013\n",
      "   0.66105516 -0.21629269]\n",
      " [ 0.5561423  -2.27664162 -0.22577993  0.28111198  0.53082793 -1.00733069\n",
      "  -2.64668928 -0.10753224]\n",
      " [-1.30844402 -1.84256786 -0.1820663  -0.92429569  1.05429876  0.35076997\n",
      "   0.70593623  0.91671197]\n",
      " [ 0.2805147   0.28901506  0.22105733 -0.40619189  1.85258332  0.35316787\n",
      "   0.23809994  0.59462629]]\n"
     ]
    }
   ],
   "source": [
    "print(\"===================== Query Vector q =========================\")\n",
    "print(q)\n",
    "print(\"===================== Key Vector k ===========================\")\n",
    "print(k)\n",
    "print(\"===================== Value Vector v =========================\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention \n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.83617689,  0.97904083,  0.38044587, -4.23919907],\n",
       "       [ 0.95974443,  6.65884549,  1.59451584, -4.59071041],\n",
       "       [-0.58514505, -3.78596764, -2.04737915,  5.48934073],\n",
       "       [-0.02435007, -2.64485003,  1.60925565,  3.43697   ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8353093225875383, 0.9690186755983412, 10.10051644599982)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why we need sqrt(dim_k) in denominator\n",
    "q.var() , k.var() , np.matmul(q,k.T).var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8353093225875383, 0.9690186755983412, 1.2625645557499774)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So inorder to reduce variance we use sqrt in denominator \n",
    "scaled = (np.matmul(q,k.T))/np.sqrt(dim_k)\n",
    "q.var() , k.var() , scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.64918657,  0.3461432 ,  0.13450793, -1.49878321],\n",
       "       [ 0.3393209 ,  2.3542574 ,  0.56374648, -1.62306123],\n",
       "       [-0.20688001, -1.3385417 , -0.72385784,  1.94077503],\n",
       "       [-0.00860905, -0.9350957 ,  0.56895779,  1.2151524 ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "- Masking is done in decoders so that  words don't get context from words generated in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.64918657,        -inf,        -inf,        -inf],\n",
       "       [ 0.3393209 ,  2.3542574 ,        -inf,        -inf],\n",
       "       [-0.20688001, -1.3385417 , -0.72385784,        -inf],\n",
       "       [-0.00860905, -0.9350957 ,  0.56895779,  1.2151524 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    #Compute softmax values for each sets of scores in x.\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14003465, 0.        , 0.        , 0.        ],\n",
       "       [0.376304  , 0.94145988, 0.        , 0.        ],\n",
       "       [0.21793491, 0.02344442, 0.21537662, 0.        ],\n",
       "       [0.26572645, 0.0350957 , 0.78462338, 1.        ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled+mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06975494, -0.01611581,  0.11355793,  0.01523868, -0.07319666,\n",
       "         0.16891261,  0.09257063, -0.03028847],\n",
       "       [ 0.33613874, -2.18667348,  0.09259248,  0.30560536,  0.30305761,\n",
       "        -0.49445597, -2.24299406, -0.18262909],\n",
       "       [-0.37732892, -0.4753015 ,  0.13222328, -0.16876532,  0.12560082,\n",
       "         0.31480883,  0.23405905,  0.14777957],\n",
       "       [-0.8589682 , -1.26718809,  0.28576481, -1.09263352,  2.55954429,\n",
       "         0.91356181,  0.87476643,  1.25265134]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.27710041  0.03188016  1.44596882 -1.34205436  0.14321269  0.27645839\n",
      "   0.2120455  -0.03424386]\n",
      " [-1.68562268  1.35760768  0.30797969 -2.00162779 -0.88145719 -1.16567825\n",
      "  -0.42736467 -0.86933783]\n",
      " [ 0.97788076 -0.08125846 -1.12267725  1.27057746 -0.03749489  0.37929365\n",
      "  -1.45820779  0.2642624 ]\n",
      " [ 0.74134556 -1.02667713 -0.48092183  1.50016204 -0.18407928  0.49035576\n",
      "   0.43729467 -0.60433436]]\n",
      "K\n",
      " [[-1.09750997  0.60600674 -1.34702431 -0.00707761 -0.32464676  0.87536513\n",
      "   0.93560748  0.20497881]\n",
      " [-1.20536592  0.7792677   0.29531355 -0.80187422 -1.30155823 -0.62707862\n",
      "   0.63349846 -0.30552488]\n",
      " [-0.61109375 -1.48863168  1.2227034   0.50665623 -2.35235382 -0.86774457\n",
      "   0.35195043 -0.33173589]\n",
      " [ 0.30374713 -1.57928372 -0.63721208  1.94223431  0.32633268 -1.55898264\n",
      "  -1.68295535  0.11461672]]\n",
      "V\n",
      " [[-0.4981263  -0.11508445  0.8109274   0.10882082 -0.52270394  1.20622013\n",
      "   0.66105516 -0.21629269]\n",
      " [ 0.5561423  -2.27664162 -0.22577993  0.28111198  0.53082793 -1.00733069\n",
      "  -2.64668928 -0.10753224]\n",
      " [-1.30844402 -1.84256786 -0.1820663  -0.92429569  1.05429876  0.35076997\n",
      "   0.70593623  0.91671197]\n",
      " [ 0.2805147   0.28901506  0.22105733 -0.40619189  1.85258332  0.35316787\n",
      "   0.23809994  0.59462629]]\n",
      "New V\n",
      " [[-0.4981263  -0.11508445  0.8109274   0.10882082 -0.52270394  1.20622013\n",
      "   0.66105516 -0.21629269]\n",
      " [ 0.43211436 -2.02234829 -0.10381797  0.26084303  0.40688666 -0.74692064\n",
      "  -2.25755437 -0.1203272 ]\n",
      " [-0.57276098 -1.01523726  0.32808981 -0.18328869  0.14445666  0.56833469\n",
      "   0.11906811  0.15409609]\n",
      " [-0.25167892 -0.50425215  0.17464274 -0.42686193  1.19566698  0.40031292\n",
      "   0.25547529  0.51631894]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.11764358 0.88235642 0.         0.        ]\n",
      " [0.52115446 0.16807071 0.31077483 0.        ]\n",
      " [0.1520313  0.06019563 0.27087387 0.5168992 ]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
